import:
    - "experiments/fewnet/base_toy_dataset.yaml"
package: []
define:
  - name: 'Experiment'
    class: Experiment
    structure: 
        class: Structure
        builder:   # 针对 fewnet 做一些更改
            class: Builder
            model: SegDetectorModel
            model_args:
                backbone: deformable_resnet18
                decoder: build_fewnet
                decoder_args:
                    need_conv_fpn: False
                    inner_channels: 32
                    num_encoder_layer: 2
                    model_dim: 32
                    nhead: 4
                loss_class: FewNetLoss
                loss_kwargs:
                    weight_cost_logits: 0
                    weight_cost_boxes: 1  # stress on boxes

                    weight_loss_score_map: 1 # 5
                    weight_loss_logits: 0  # 3
                    weight_loss_rbox: 5   # 10


        representer:
            class: FewNetPostProcess  # FewNetPostProcess
            vis_score_map: True  # need visualize score_map
        measurer:
            class: QuadMeasurer
        visualizer:  
            class: SegDetectorVisualizer
    train: 
        class: TrainSettings
        data_loader: 
            class: DataLoader
            dataset: ^train_data
            batch_size: 16
            num_workers: 16
            collect_fn:
                class: FewNetCollate  # define user-defined collate
        checkpoint:
            class: Checkpoint
            start_epoch: 0
            start_iter: 0
            resume: null
        model_saver: 
            class: ModelSaver
            dir_path: model
            save_interval: 18000
            signal_path: save
        scheduler: # How to define multiple learning rate ?
            class: FewNetOptimizerScheduler
            optimizer: "AdamW"  # official optimizer is sufficient
            optimizer_args:  # lr in optimizer is useless due to the pipeline of project
                constructor_args: { lr: 0.0001, weight_decay: 0.0001 }  # default args
                params_dict: # For other params
                    - {
                        params_key: "feature_grouping",
                        lr: 0.0001, weight_decay: 0.0001
                    }

            learning_rate: FewNetScheduler  # 针对不同的 param_group, 定义 不同的 scheduler
            learning_rate_args: # order should keep pace with optimizer
                constructor: DecayLearningRate
                constructor_args: { lr: 0.001, epochs: 50 }
                feature_grouping: # keep pace to optimizer_args.params_dict[i].params_key
                    constructor: FewNetLearningRate
                    constructor_args: { lr: 0.007 }  # epoch is the same as default
        epochs: 50

    validation: &validate
        class: ValidationSettings
        data_loaders:
            icdar2015: 
                class: DataLoader
                dataset: ^validate_data
                batch_size: 1
                num_workers: 16
                collect_fn:
                    class: ICDARCollectFN
        visualize: false
        interval: 4500
        exempt: 1

    logger:
        class: Logger
        verbose: true
        level: info
        log_interval: 2

    evaluation: *validate
