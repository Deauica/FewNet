import:
    - "experiments/fewnet/base_td500.yaml"
package: []
define:
  - name: 'Experiment'
    class: Experiment
    structure: 
        class: Structure
        builder:   # 针对 fewnet 做一些更改
            class: Builder
            model: SegDetectorModel
            model_args:
                backbone: deformable_resnet50

                decoder: build_fewnet
                decoder_args:
                    in_channels: [256, 512, 1024, 2048]
                    nhead: 16
                    model_dim: 512
                    num_encoder_layer: 4

                loss_class: FewNetLoss
                loss_kwargs:
                    weight_cost_logits: 0
                    weight_cost_boxes: 1  # stress on boxes

                    weight_loss_score_map: 1 # 5
                    weight_loss_logits: 0  # 3
                    weight_loss_rbox: 5   # 10

        representer:
            class: FewNetPostProcess  # FewNetPostProcess
            vis_score_map: True  # need visualize score_map
        measurer:
            class: QuadMeasurer
        visualizer:  
            class: SegDetectorVisualizer
    train: 
        class: TrainSettings
        data_loader: 
            class: DataLoader
            dataset: ^train_data
            batch_size: 16
            num_workers: 16
            collect_fn:
                class: FewNetCollate  # define user-defined collate
        checkpoint:
            class: Checkpoint
            start_epoch: 0
            start_iter: 0
            resume: null
        model_saver: 
            class: ModelSaver
            dir_path: model
            save_interval: 450  # save interval 450
            signal_path: save
        scheduler: # How to define multiple learning rate ?
            class: FewNetOptimizerScheduler
            optimizer: "AdamW"  # official optimizer is sufficient
            optimizer_args: # lr in optimizer is useless due to the pipeline of project
                constructor_args: { lr: 0.0001, weight_decay: 0.0001 }  # default args
                params_dict: # For other params
                    - {
                        params_key: "feature_grouping",
                        lr: 0.0001, weight_decay: 0.0001
                    }

            learning_rate: FewNetScheduler  # 针对不同的 param_group, 定义 不同的 scheduler
            learning_rate_args: # order should keep pace with optimizer
                constructor: DecayLearningRate
                constructor_args: { lr: 0.0001, epochs: 1200 }
                feature_grouping: # keep pace to optimizer_args.params_dict[i].params_key
                    constructor: DecayLearningRate
                    constructor_args: { lr: 0.001 }  # epoch is the same as default
        epochs: 1200

    validation: &validate
        class: ValidationSettings
        data_loaders:
            icdar2015: 
                class: DataLoader
                dataset: ^validate_data
                batch_size: 1
                num_workers: 16
                collect_fn:
                    class: ICDARCollectFN  # Collate fn is changed
        visualize: false
        interval: 4500
        exempt: 1

    logger:
        class: Logger
        verbose: true
        level: info
        log_interval: 5

    evaluation: *validate
